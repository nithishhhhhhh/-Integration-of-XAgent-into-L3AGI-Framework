{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbUzSe1dP2uHfQxij3wGrB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD7Vv1yOh3YS",
        "outputId": "f943cd0f-c951-472d-c44c-9800ae81b0a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'L3AGI' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/l3vels/L3AGI.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/L3AGI/apps/server/test.py\n",
        "from xagent import XAgent, XAgentClient, XAgentConfig, XAgentEvalConfig\n",
        "from xagent.tools import XSerpGoogleSearch\n",
        "from langsmith import Client\n",
        "\n",
        "# Refactored function to initialize the XAgent\n",
        "def xagent_factory():\n",
        "    # Initialize the XAgent with necessary configurations\n",
        "    xagent = XAgent(\n",
        "        model_name=\"gpt-3.5-turbo\",\n",
        "        temperature=0,\n",
        "        tools=[XSerpGoogleSearch()],\n",
        "        system_message=\"Your system message here\",\n",
        "        output_parser=\"Your output parser here\",\n",
        "        max_iterations=5,\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=\"Check your output and make sure it conforms!\"\n",
        "    )\n",
        "    return xagent\n",
        "\n",
        "# Initialize the agent\n",
        "agent = xagent_factory()\n",
        "\n",
        "# Initialize the Langsmith client (assuming it can interact with XAgent)\n",
        "client = Client()\n",
        "\n",
        "# Evaluation configuration using XAgent's configuration class\n",
        "eval_config = XAgentEvalConfig(\n",
        "    evaluators=[\n",
        "        \"qa\",\n",
        "        XAgentEvalConfig.Criteria(\"helpfulness\"),\n",
        "        XAgentEvalConfig.Criteria(\"conciseness\"),\n",
        "    ],\n",
        "    input_key=\"input\",\n",
        "    eval_llm=XAgent(model_name=\"gpt-3.5-turbo\", temperature=0.5),\n",
        ")\n",
        "\n",
        "# Running evaluation on the dataset with XAgent\n",
        "chain_results = client.run_on_dataset(\n",
        "    dataset_name=\"test-dataset\",\n",
        "    llm_or_chain_factory=xagent_factory,\n",
        "    evaluation=eval_config,\n",
        "    concurrency_level=1,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR8pXnMyVHAO",
        "outputId": "901779a8-b508-4c3b-b214-5871754aca43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/L3AGI/apps/server/test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/L3AGI/apps/server/agents/conversational/conversational.py\n",
        "import asyncio\n",
        "\n",
        "from xagent import XAgent, XAgentExecutor, XAgentConfig, XAgentCallbackHandler, XAgentErrorHandling\n",
        "from xagent.tools import XSerpGoogleSearch, speech_to_text, text_to_speech\n",
        "from xagent.memory import XAgentMemory\n",
        "from xagent.output_parser import XAgentOutputParser\n",
        "from xagent.streaming_aiter import XAgentAsyncCallbackHandler\n",
        "from config import Config\n",
        "from memory.zep.zep_memory import ZepMemory\n",
        "from postgres import PostgresChatMessageHistory\n",
        "from services.pubsub import ChatPubSubService\n",
        "from services.run_log import RunLogsManager\n",
        "from typings.agent import AgentWithConfigsOutput\n",
        "from typings.config import AccountSettings, AccountVoiceSettings\n",
        "from utils.model import get_llm\n",
        "from utils.system_message import SystemMessageBuilder\n",
        "\n",
        "class ConversationalAgent(BaseAgent):\n",
        "    async def run(\n",
        "        self,\n",
        "        settings: AccountSettings,\n",
        "        voice_settings: AccountVoiceSettings,\n",
        "        chat_pubsub_service: ChatPubSubService,\n",
        "        agent_with_configs: AgentWithConfigsOutput,\n",
        "        tools,\n",
        "        prompt: str,\n",
        "        voice_url: str,\n",
        "        history: PostgresChatMessageHistory,\n",
        "        human_message_id: str,\n",
        "        run_logs_manager: RunLogsManager,\n",
        "        pre_retrieved_context: str,\n",
        "    ):\n",
        "        memory = XAgentMemory(\n",
        "            session_id=str(self.session_id),\n",
        "            url=Config.ZEP_API_URL,\n",
        "            api_key=Config.ZEP_API_KEY,\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "        )\n",
        "\n",
        "        memory.human_name = self.sender_name\n",
        "        memory.ai_name = agent_with_configs.agent.name\n",
        "\n",
        "        system_message = SystemMessageBuilder(\n",
        "            agent_with_configs, pre_retrieved_context\n",
        "        ).build()\n",
        "\n",
        "        res: str\n",
        "\n",
        "        try:\n",
        "            if voice_url:\n",
        "                configs = agent_with_configs.configs\n",
        "                prompt = speech_to_text(voice_url, configs, voice_settings)\n",
        "\n",
        "            llm = get_llm(\n",
        "                settings,\n",
        "                agent_with_configs,\n",
        "            )\n",
        "\n",
        "            streaming_handler = XAgentAsyncCallbackHandler()\n",
        "\n",
        "            llm.streaming = True\n",
        "            # llm.callbacks = [\n",
        "            #     run_logs_manager.get_agent_callback_handler(),\n",
        "            #     streaming_handler,\n",
        "            # ]\n",
        "\n",
        "            # agent = initialize_agent(\n",
        "            #     tools,\n",
        "            #     llm,\n",
        "            #     agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "            #     verbose=True,\n",
        "            #     memory=memory,\n",
        "            #     handle_parsing_errors=\"Check your output and make sure it conforms!\",\n",
        "            #     agent_kwargs={\n",
        "            #         \"system_message\": system_message,\n",
        "            #         \"output_parser\": XAgentOutputParser(),\n",
        "            #     },\n",
        "            #     callbacks=[run_logs_manager.get_agent_callback_handler()],\n",
        "            # )\n",
        "\n",
        "            agent = XAgent(\n",
        "                model_name=\"gpt-3.5-turbo\",\n",
        "                temperature=0,\n",
        "                tools=tools,\n",
        "                system_message=system_message,\n",
        "                output_parser=XAgentOutputParser(),\n",
        "                max_iterations=5,\n",
        "                verbose=True,\n",
        "                handle_parsing_errors=\"Check your output and make sure it conforms!\",\n",
        "            )\n",
        "\n",
        "            agent_executor = XAgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "            chunks = []\n",
        "            final_answer_detected = False\n",
        "\n",
        "            async for event in agent_executor.astream_events(\n",
        "                {\"input\": prompt}, version=\"v1\"\n",
        "            ):\n",
        "                kind = event[\"event\"]\n",
        "\n",
        "                if kind == \"on_chat_model_stream\":\n",
        "                    content = event[\"data\"][\"chunk\"].content\n",
        "                    if content:\n",
        "                        chunks.append(content)\n",
        "                        # Check if the last three elements in chunks,\n",
        "                        # when stripped, are \"Final\", \"Answer\", and \":\"\n",
        "                        if (\n",
        "                            len(chunks) >= 3\n",
        "                            and chunks[-3].strip() == \"Final\"\n",
        "                            and chunks[-2].strip() == \"Answer\"\n",
        "                            and chunks[-1].strip() == \":\"\n",
        "                        ):\n",
        "                            final_answer_detected = True\n",
        "                            continue\n",
        "\n",
        "                        if final_answer_detected:\n",
        "                            yield content\n",
        "\n",
        "            full_response = \"\".join(chunks)\n",
        "            final_answer_index = full_response.find(\"Final Answer:\")\n",
        "            if final_answer_index != -1:\n",
        "                # Add the length of the phrase \"Final Answer:\"\n",
        "                # and any subsequent whitespace or characters you want to skip\n",
        "                start_index = final_answer_index + len(\"Final Answer:\")\n",
        "                # Optionally strip leading whitespace\n",
        "                res = full_response[start_index:].lstrip()\n",
        "            else:\n",
        "                res = \"Final Answer not found in response.\"\n",
        "\n",
        "        except Exception as err:\n",
        "            res = XAgentErrorHandling.handle_agent_error(err)\n",
        "\n",
        "            memory.save_context(\n",
        "                {\n",
        "                    \"input\": prompt,\n",
        "                    \"chat_history\": memory.load_memory_variables({})[\"chat_history\"],\n",
        "                },\n",
        "                {\n",
        "                    \"output\": res,\n",
        "                },\n",
        "            )\n",
        "\n",
        "            yield res\n",
        "\n",
        "        try:\n",
        "            configs = agent_with_configs.configs\n",
        "            voice_url = None\n",
        "            if \"Voice\" in configs.response_mode:\n",
        "                voice_url = text_to_speech(res, configs, voice_settings)\n",
        "                pass\n",
        "        except Exception as err:\n",
        "            res = f\"{res}\\n\\n{XAgentErrorHandling.handle_agent_error(err)}\"\n",
        "\n",
        "            yield res\n",
        "\n",
        "        ai_message = history.create_ai_message(\n",
        "            res,\n",
        "            human_message_id,\n",
        "            agent_with_configs.agent.id,\n",
        "            voice_url,\n",
        "        )\n",
        "\n",
        "        chat_pubsub_service.send_chat_message(chat_message=ai_message)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFFgoPSZVG0V",
        "outputId": "86954531-b552-4c84-8420-49c0adf8fedf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/L3AGI/apps/server/agents/conversational/conversational.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/L3AGI/apps/server/agents/agent_simulations/agent/dialogue_agent_with_tools.py\n",
        "from typing import List, Optional\n",
        "\n",
        "from xagent import XAgent, XAgentExecutor, XAgentConfig, XAgentCallbackHandler, XAgentErrorHandling\n",
        "from xagent.memory import XAgentMemory\n",
        "from xagent.output_parser import XAgentOutputParser\n",
        "from config import Config\n",
        "from services.run_log import RunLogsManager\n",
        "from typings.agent import AgentWithConfigsOutput\n",
        "\n",
        "\n",
        "class DialogueAgentWithTools(DialogueAgent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        agent_with_configs: AgentWithConfigsOutput,\n",
        "        system_message: SystemMessage,\n",
        "        model: XAgent,  # Replace ChatOpenAI with XAgent\n",
        "        tools: List[any],\n",
        "        session_id: str,\n",
        "        sender_name: str,\n",
        "        is_memory: bool = False,\n",
        "        run_logs_manager: Optional[RunLogsManager] = None,\n",
        "        **tool_kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(name, agent_with_configs, system_message, model)\n",
        "        self.tools = tools\n",
        "        self.session_id = session_id\n",
        "        self.sender_name = sender_name\n",
        "        self.is_memory = is_memory\n",
        "        self.run_logs_manager = run_logs_manager\n",
        "\n",
        "    def send(self) -> str:\n",
        "        \"\"\"\n",
        "        Applies the chatmodel to the message history\n",
        "        and returns the message string\n",
        "        \"\"\"\n",
        "\n",
        "        memory: XAgentMemory\n",
        "\n",
        "        # FIXME: This is a hack to get the memory working\n",
        "        if self.is_memory:\n",
        "            memory = XAgentMemory(\n",
        "                session_id=self.session_id,\n",
        "                url=Config.ZEP_API_URL,\n",
        "                api_key=Config.ZEP_API_KEY,\n",
        "                memory_key=\"chat_history\",\n",
        "                return_messages=True,\n",
        "            )\n",
        "\n",
        "            memory.human_name = self.sender_name\n",
        "            memory.ai_name = self.agent_with_configs.agent.name\n",
        "            memory.auto_save = False\n",
        "        # else:\n",
        "        #     memory = ConversationBufferMemory(\n",
        "        #         memory_key=\"chat_history\", return_messages=True\n",
        "        #     )\n",
        "\n",
        "        callbacks = []\n",
        "\n",
        "        if self.run_logs_manager:\n",
        "            self.model.callbacks = [self.run_logs_manager.get_agent_callback_handler()]\n",
        "            callbacks.append(self.run_logs_manager.get_agent_callback_handler())\n",
        "\n",
        "        agent = XAgent(\n",
        "            model_name=\"gpt-3.5-turbo\",\n",
        "            temperature=0,\n",
        "            tools=self.tools,\n",
        "            system_message=self.system_message.content,\n",
        "            output_parser=XAgentOutputParser(),\n",
        "            max_iterations=5,\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=\"Check your output and make sure it conforms!\",\n",
        "            memory=memory,\n",
        "            callbacks=callbacks,\n",
        "        )\n",
        "\n",
        "        agent_executor = XAgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "\n",
        "        prompt = \"\\n\".join(self.message_history + [self.prefix])\n",
        "\n",
        "        res = agent_executor.run(input=prompt)\n",
        "\n",
        "        # FIXME: is memory\n",
        "        # memory.save_ai_message(res)\n",
        "\n",
        "        message = AIMessage(content=res)\n",
        "\n",
        "        return message.content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k885_Fs1VGwe",
        "outputId": "ed9a7670-7470-46e7-c36c-5b0c72aaa2b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/L3AGI/apps/server/agents/agent_simulations/agent/dialogue_agent_with_tools.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfYoiK20VGub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wyylCX0gVGql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKVJayIoVGom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mxgIvm6VGiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vUkOtz1UVGei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "le8csMN3VGcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQlRZ0F-VGZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1ZKNXHgVGXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ey1NM5gGVGTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6m0bCmHVGRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZ5dCXaZVGMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lOEv9abVGId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpQx0fZLVGGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaZZMbqzVGB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}